---
title: "Analyzing Fairness: Race, Recidivism, and the COMPAS Algorithm"
author: "Elisabeth Nesmith, Eunice Kim, Emma Kornberg, Amrita Acharya, Dianne Caravela"
date: "3/8/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Set Up

```{r py setup, include=FALSE}
library(reticulate)
#use_condaenv("anaconda3")
```

```{python import, include= FALSE}
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import pandas as pd
import seaborn as sns

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

from sklearn.compose import make_column_transformer
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.preprocessing import OneHotEncoder

from aif360.datasets import CompasDataset
from aif360.datasets import BinaryLabelDataset
from aif360.sklearn.datasets import fetch_compas
from aif360.algorithms.inprocessing import MetaFairClassifier, PrejudiceRemover, ARTClassifier


from aif360.sklearn.preprocessing import ReweighingMeta
from aif360.sklearn.postprocessing import CalibratedEqualizedOdds, PostProcessingMeta

from aif360.sklearn.metrics import disparate_impact_ratio, average_odds_difference, statistical_parity_difference, equal_opportunity_difference

# from aif360.algorithms.preprocessing import DisparateImpactRemover
# from BlackBoxAuditing.repairers.GeneralRepairer import Repairer

from IPython.display import Markdown, display
```

# Data

```{python fetch compas}
## load in data
## In machine learning tasks, specifically with supervised learning, you have features and labels. 
## The features are the descriptive attributes (they are defined as X), and the label (y) is what you're attempting to predict or forecast

X, y = fetch_compas()
print(f'There are {X.shape[0]} entries and {X.shape[1]} features')
X.head()
```

## Filter/configure race

```{python filter race}
## because our analysis is mainly focusing on how the algorithm treats white and Black people differently, we are
## dropping the rows of data where race != Caucasian or African American
X_new = X[(X.race == "Caucasian") | (X.race == "African-American")]
print(f'There are {X_new.shape[0]} entries and {X_new.shape[1]} features')
X_new.head()
```

```{python drop other races}
## drop unused race categories
# list of categories to be removed
X_new["race"] = X_new["race"].cat.remove_unused_categories()
```

```{python configure races}
y_new = y[(y.index.get_level_values(2) == "Caucasian") | (y.index.get_level_values(2) == "African-American")]
y_new.head()
```

```{python initial confusion matrix}
# Function for visualising the confusion matrix and other statistics
# https://github.com/DTrimarchi10/confusion_matrix/blob/master/cf_matrix.py

def make_confusion_matrix(cf_matrix, model):
  group_names = ["True Negative","False Positive","False Negative","True Positive"]
  group_counts = ["{0:0.0f}".format(value) for value in
                  cf_matrix.flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in
                      cf_matrix.flatten()/np.sum(cf_matrix)]

  group_labels = ["{}\n".format(value) for value in group_names]
  group_counts = ["{0:0.0f}\n".format(value) for value in cf_matrix.flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]

  box_labels = [f"{v1}{v2}{v3}".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]
  box_labels = np.asarray(box_labels).reshape(cf_matrix.shape[0],cf_matrix.shape[1])


  # add more statistics
  accuracy  = np.trace(cf_matrix) / float(np.sum(cf_matrix))
  precision = cf_matrix[1,1] / sum(cf_matrix[:,1])
  recall    = cf_matrix[1,1] / sum(cf_matrix[1,:])
  f1_score  = 2*precision*recall / (precision + recall)
  stats_text = "\n\nAccuracy={:0.3f}\nPrecision={:0.3f}\nRecall={:0.3f}\nF1 Score={:0.3f}".format(
      accuracy,precision,recall,f1_score)


  categories=["Survived", "Recidivated"]
  sns.heatmap(cf_matrix,annot=box_labels,fmt="",cmap='Purples',xticklabels=categories,yticklabels=categories)

  plt.ylabel('True label')
  plt.xlabel('Predicted label' + stats_text)
  plt.title(f"Confusion matrix and statistics for the {model} model");

## defining function for displaying metrics of training and test data by race
def metrics_per_group(y_test, y_pred):
	# y true per group
	y_test_white = y_test.loc[y_test.index.get_level_values(2) == 1]
	y_test_black = y_test.loc[y_test.index.get_level_values(2) == 0]

	# y_pred per group
	y_pred_white = y_pred[y_test.index.get_level_values(2) == 1]
	y_pred_black = y_pred[y_test.index.get_level_values(2) == 0]

	# metrics
	scores = []
	scores.append(accuracy_score(y_test, y_pred))
	scores.append(recall_score(y_test, y_pred))
	scores.append(precision_score(y_test, y_pred))

	scores.append(accuracy_score(y_test_black, y_pred_black))
	scores.append(recall_score(y_test_black, y_pred_black))
	scores.append(precision_score(y_test_black, y_pred_black))

	scores.append(accuracy_score(y_test_white, y_pred_white))
	scores.append(recall_score(y_test_white, y_pred_white))
	scores.append(precision_score(y_test_white, y_pred_white))

	attribute = ["all"]*3 + ["black"] *3 + ["white"] *3
	metric = ["accuracy", "recall", "precision"] * 3
	  
	# dictionary of lists 
	dict = {'race': attribute, 'metrics': metric, 'score': scores} 
	    
	df = pd.DataFrame(dict)

	sns.barplot(x = "metrics", y = "score", hue = "race", data = df, palette = ['#dfcd1a', '#9d0677', '#236c48'])
	plt.title("Performance metrics by groups")
 

def plot_fair_metrics(fair_metrics_mitigated, model): 
  cols = ['statistical_parity_difference','equal_opportunity_difference','average_odds_difference','disparate_impact_ratio']
  obj_fairness = [[0,0,0,1]]

  # row for objectives    
  fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)
      
  # row for baseline model
  fair_metrics.loc['Baseline Model'] = [stat_par_diff, eq_opp_diff, avg_odds_diff, disp_impact_ratio]

  # row for mitigated bias
  fair_metrics.loc[model] = fair_metrics_mitigated


  metrics_len = len(cols)


  fig, ax = plt.subplots(figsize=(20,4), ncols=metrics_len, nrows=1)

  plt.subplots_adjust(
      left    =  0.125, 
      bottom  =  0.1, 
      right   =  0.9, 
      top     =  0.9, 
      wspace  =  .5, 
      hspace  =  1.1
  )

  y_title_margin = 1.2

  plt.suptitle("Fairness metrics", y = 1.09, fontsize=20)
  sns.set(style="dark")

  cols = fair_metrics.columns.values
  obj = fair_metrics.loc['objective']
  size_rect = [0.2,0.2,0.2,0.4]
  rect = [-0.1,-0.1,-0.1,0.8]
  bottom = [-1,-1,-1,0]
  top = [1,1,1,2]
  bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.25]]

  for i in range(0,metrics_len):
      plt.subplot(1, metrics_len, i+1)
      ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])
      
      for j in range(0,len(fair_metrics)-1):
          a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]
          marg = -0.2 if val < 0 else 0.1
          ax.text(a.get_x()+a.get_width()/5, a.get_y()+a.get_height()+marg, round(val, 3), fontsize=15,color='black')

      plt.ylim(bottom[i], top[i])
      plt.setp(ax.patches, linewidth=0)
      ax.add_patch(patches.Rectangle((-5,rect[i]), 10, size_rect[i], alpha=0.3, facecolor="green", linewidth=1, linestyle='solid'))
      plt.axhline(obj[i], color='black', alpha=0.3)
      plt.title(cols[i])
      ax.set_ylabel('')    
      ax.set_xlabel('')
```

```{python new race index}
X_new.index = pd.MultiIndex.from_arrays(X_new.index.codes, names=X_new.index.names)
y_new.index = pd.MultiIndex.from_arrays(y_new.index.codes, names=y_new.index.names)
# 0 is African American, 2 is Caucasian
```

```{python re-index race}
# set caucasian equal to 1 instead of 2
X_new = X_new.rename(index={2: 1}, level='race')
X_new
```

```{python target class}
# set target class to 0/1 
y_new = pd.Series(y_new.factorize(sort=True)[0], index=y_new.index)
# set caucasian equal to 1 instead of 2
y_new = y_new.rename(index={2: 1}, level='race')
```

```{python y new race}
y_new
```


# Exploratory Data Analysis
```{python EDA} 
# needs interpretation
X_new_index = X_new.rename(columns={"race": "def_race"})
X_new_index = X_new_index.rename(columns={"sex": "def_sex"})
# renaming dataframe columns to avoid the ValueError of variables being a column AND index label (which is ambiguous)

X_new_index.groupby(["def_race"])["age"].median()
X_new_index.groupby(["def_race", "sex"]).size()
X_new_index.groupby(["def_race", "c_charge_degree"]).size()
X_new_index.groupby(["def_race"])["priors_count"].median()
#X_new_index.groupby(["def_race"])["juv_fel_count"].median()
#X_new_index.groupby(["def_race"])["juv_misd_count"].median()
# above two lines commented out since medians for everyone = 0

```

```{python exploratory visualization set up}
df_viz = X_new.copy()
df_viz['race'] = X_new['race'].replace({1.0: 'Caucasian', 0.0: 'African-American'})
df_viz['two_year_recid'] = y_new.replace({1:'Recidivated', 0: 'Survived'})
df_viz.index = df_viz.index.droplevel('race')

purple = '#9d0677'
green = '#30875c'
orange = '#E7881E'
blue = '#20A4CF'
workshop_palette = [purple, green]
df_viz.head()
```

```{python exploratory recidivism bar plot}
# barplot of recividism
sns.countplot(x='two_year_recid', data=df_viz, palette=workshop_palette)
plt.title('Two Year Recidivism Rate')
```

This plot shows the distribution of two year recidivism outcomes. Out of all defendants, there is a higher proportion of people who did not recidivate than who did. About 2500 people recidivated, whereas approximately 2700 did not.

```{python exploratory race bar plot}
# barplot of race
sns.countplot(x="race", data=df_viz, palette=workshop_palette)
plt.title('Race Distribution (White and Black individuals only)')
```

This plot shows the distribution of defendant races. The dataset contains about 3200 Black defendants and 2100 white defendants. Therefore, there are more Black people represented in the dataset than white people, showing the disproportionate policing and criminalization of Black communities.

```{python exploratory age by race plot}
# age distribution by race
#ax = sns.kdeplot(x="age", hue="race", data=df_viz, palette=workshop_palette);
#kdeline = ax.lines[0]
#mean_black = df_viz.groupby('race').age.median()[0]
#xs = kdeline.get_xdata()
#ys = kdeline.get_ydata()
#height_black = np.interp(mean_black, xs, ys)
#ax.vlines(mean_black, 0, height_black, color=purple, ls=':')

#mean_white = df_viz.groupby('race').age.median()[1]
#height_white = np.interp(mean_white, xs, ys)
#ax.vlines(mean_white, 0, height_white, color=green, ls=':')


#print(df_viz.groupby('race').age.median())

#plt.title('Distribution of Age by Race')
```

This plot shows the distribution of defendant ages by race. The purple curve shows the distribution of the ages of Black defendants, and the green curve shows the distribution of the ages of white defendants. The probability of a defendant's age being between two points on the x-axis is the total shaded area of the curve under the two points. The purple dotted line represents the median age of Black defendants (29 years) and the green dotted line represents the median age of white defendants (35 years). For both groups, the majority of defendants are relatively young, but this is especially noticeable for Black defendants.

```{python exploratory recidivism by race plot}
# recidivism by race
by_sex = sns.countplot(x="race", hue="two_year_recid", data=df_viz, palette=workshop_palette)

plt.title('Two Year Recidivism by Race')
plt.show()
```

This plot shows the distribution of two year recidivism outcomes by race. When we divide the data into Black and white defendants, we can see that Black defendants recidivate more than white defendants and Black defendants are more likely to recidivate than not recidivate. Again, this trend points to disproportionate rates of criminalization of Black people. This increased likelihood of recidivating may be attributed to the systemic racism in policing (e.g., predominantly Black neighborhoods tend to be overpoliced in comparison to predominantly white neighborhoods). 

```{python exploratory recidivism by race table}
# table of recidivism by race
pd.crosstab(index = df_viz["race"], columns = df_viz["two_year_recid"])
```

This table shows the counts of recidivism by race. 

```{python exploratory priors by race}
priors_by_race = sns.countplot(x="race", color="priors_count", data=df_viz, palette=workshop_palette)

plt.title('Priors Count by Race')
plt.show()
```

```{python exploratory priors by sex}
priors_by_race = sns.countplot(x="sex", color="priors_count", data=df_viz, palette=workshop_palette)

plt.title('Priors Count by Sex')
plt.show()
```

The two Priors Count bar charts show the differences in the distribution of priors by race and sex. If a defendant has a higher number of prior offenses, the individual is more likely to be rated with a higher risk of recidivism. In the former plot, one can see that Black defendants have a higher number of priors than do white defendants. This again likely reflects the over-policing of Black neighborhoods. In the latter plot, one can see that male defendants have a higher number of priors than do female defendants. This is worth noting because the COMPAS risk assessment algorithm has been found to express a discrepancy in prediction between sexes. When the recidivsim predictions are compared with the true recividism outcomes, female defendants have a notable false positive rate that predicts their recidivism risk as higher than their true outcomes.

# Baseline Metrics

```{python baseline train and test}
X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, random_state=1234567)

# one-hoy encode the categorical features
data_preproc = make_column_transformer(
        (OneHotEncoder(sparse=False, handle_unknown='ignore'), X_train.dtypes == 'category'))

X_train = pd.DataFrame(data_preproc.fit_transform(X_train), index=X_train.index)
X_test = pd.DataFrame(data_preproc.transform(X_test), index=X_test.index)

# to save the information for the column names
pd.get_dummies(X_new).head()
```

```{python baseline set up, include = FALSE}
from aif360.sklearn.preprocessing import ReweighingMeta, Reweighing
from sklearn.metrics import accuracy_score, confusion_matrix, plot_roc_curve, recall_score, precision_score


lr = LogisticRegressionCV(solver='lbfgs')
reg = lr.fit(X_train, y_train)
y_pred = reg.predict(X_test)

acc_base = accuracy_score(y_test, y_pred)
print(f'[Baseline] The test accuracy of the algorithm is: {acc_base: .2%}')
```

```{python baseline confusion matrix}
##Look at 2nd column of matrix for precision 
##2nd row of matrix for recall 
cf_matrix = confusion_matrix(y_test, y_pred)
make_confusion_matrix(cf_matrix, "[Baseline]")
plt.show()
```

The highest percentage in this confusion matrix is for true negatives, defendants who the model predicted to not recidivate and actually did not, at 35.41%. The number of true positives, defendants who the model predicted to recidivate and actually recidivated, is slightly lower at 25.85%. The percentages of false negatives and false positives, which represent incorrect model predictions, are somewhat high for this baseline model. This indicates that the model might need bias mitigation or improvements in accuracy. 

```{python baseline metrics per group}
metrics_per_group(y_test, y_pred)
plt.show()
```

This graphs shows three different metrics of model quality and how they differ by race. Accuracy is calculated by summing the total number of true positives and true negatives and dividing by the total number of predictions:

$$\frac{TP + TN}{TP + FP + TN + FN} $$

Accuracy, as the name suggests, quantifies how accurately the model makes predictions. Looking at the graph, we can see that accuracy is fairly similar for Black, white, and all defendants, but the model is slightly less accurate in the case of Black defendants. The model makes accurate predictions for all defendants about 61% of the time, for Black defendants about 59% of the time, and for white defendants about 63% of the time. 

The value of recall represents the proportion of actual positives which were predicted correctly. In other words, recall is the number of people who were predicted to recidivate and did recidivate out of the total number of people actually recidivated:

$$\frac{TP}{TP + FN}$$

In the case of our baseline mode, recall is highest for Black defendants at nearly 0.7, while recall for white defendants is much lower at just above 0.2. The model correctly identifies defendants who recidivate about 54% of the time, Black defendants who recidivate about 70% of the time, and white defendants who recidivate about 20% of the time. This means that the model frequently predicts white defendants to not recidivate (the more favorable outcome) when they actually do. This points to potential bias in the model, as the recall score for white defendants is so much lower than that for Black defendants.

Slightly different from recall, precision is the proportion of predicted positives which were actually correct. Precision is therefore the number of people who were predicted to recidivate and did recidivate out of the total number of people who were predicted to recidivate:

$$\frac{TP}{TP + FP} $$ 
We can see that precision is very similar across groups, all approximately 0.6. This means that the model's predictions of a defendant, either Black or white, who recidivates are accurate about 60% of the time.

White people have a lower recall rate, meaning that there is a low true positive rate. Whereas for black people the true positive rate is much higher: there's a high percentage of recidivated guesses that were correct.

## Group Fairness Metrics

With group fairness metrics, different groups should receive similar treatments or outcomes. In this case of recidivism and race, this means that Black defendants should have similar rates of predicted recidivism as white defendants.

### Statistical Parity Difference 

Statistical Parity difference is computed as the difference in the rate of favorable outcomes (in this case, did not recidivate) received by the unprivileged group to the privileged group. It essentially equalizes the outcomes across the privileged and non-privileged groups.
The ideal value of this metric is 0. Fairness for this metric is between -0.1 and 0.1. A negative value means there is higher benefit for the privileged group (in this case, white defendants).

$P(\hat{Y}=1|D=Unprivileged) - P(\hat{Y}=1|D=Privileged)$

```{python group fairnesss statistical parity difference}
stat_par_diff = statistical_parity_difference(y_test, prot_attr='race', priv_group = 1, pos_label = 0)

print(f'[Baseline] The statistical parity difference is {stat_par_diff: .2}')
```

Because the statistical parity difference is negative and not within -0.1 and 0.1, the algorithm unfairly benefits white defendants.

### Disparate Impact Ratio

This metric is the ratio of how often the favorable outcome occurs in one group versus the other. In the case of recidivism, this is the ratio of how many white defendants are predicted to not recidivate compared to how many black defendants are predicted to not recidivate. 

A value of 1 means that the ratio is exactly 1:1. Less than 1 means the privileged group (white defendants) benefits, while a value greater than 1 means the unprivileged group (Black defendants) benefits. According to AI Fairness 360, a ratio between .8 to 1.25 is considered fair.

The disparate impact ratio is calculated with the formula: 

$$\frac{P(\hat{Y}=1|D=Unprivileged)}{P(\hat{Y}=1|D=Privileged)}$$ 

where Y is the favorable outcome of a defendant's predicted two year recidivism and D is the race of the defendant. In our case, the favorable outcome is survived, or not recidivated, which is Y = 0. In terms of race, 0 = Black and 1 = White. So to calculate the disparate impact ratio on this data, the formula is: 

$$\frac{P(\hat{Y}=0|D=0)}{P(\hat{Y}=0|D=1)}$$ 

Say we have a data test set of 200 defendants, 125 Black and 75 white. Of the 125 Black defendants, 44% (55) were predicted to not recidivate. Of the 75 white defendants, 66% (50) were predicted to not recidivate.

The disparate impact ratio would therefore be 

$$\frac{0.44}{0.66} = 0.667$$

Because this value is less than 1, it benefits white defendants. Additionally, because it is below 0.8, it is considered "unfair."

```{python group fairnesss disparate impact ratio}
disp_impact_ratio = disparate_impact_ratio(y_test, y_pred, prot_attr='race', priv_group = 1, pos_label = 0)

print(f'[Baseline] The disparate impact ratio is {disp_impact_ratio: .2}')
```

Since the actual disparate impact ratio for our test data is 0.47 and below 0.8, the algorithm unfairly benefits white defendants i.e. disproportionately predicts them to not recidivate.

### Average Odds Difference

This metric returns the average difference in false positive rate and true positive rate for the privileged and unprivileged groups. A value of 0 is considered fair, and a value below 0 implies benefit for the privileged group. Equality of odds is achieved in the case of recidivism when the proportion of people who were predicted to recidivate and did recidivate is equal (true positive rate) for both black and white defendants AND the proportion of people who were predicted to recidivate and did not recidivate (false positive rate) is equal for both black and white defendants. 

$$\frac{(FPR_{D = unprivileged} - FPR_{D = privileged}) + (TPR_{D = unprivileged} - TPR_{D = privileged})}{2}$$

```{python group fairnesss average odds difference}
avg_odds_diff = average_odds_difference(y_test, y_pred, prot_attr='race', pos_label = 0)

print(f'[Baseline] The average odds difference is {avg_odds_diff: .2}')
```

Since the average odds difference is below 0 by a substantial amount, white defendants are unfairly benefitted by the algorithm.

### Equal Opportunity Difference 

This metric is computed as the difference of true positive rates between the unprivileged and the privileged groups. The true positive rate is the ratio of true positives to the total number of actual positives for a given group.
The ideal value is 0. A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group.
Fairness for this metric is between -0.1 and 0.1.

$$TPR_{D = Unprivileged} - TPR_{D = Privileged}$$

```{python group fairnesss equal opportunity difference}
eq_opp_diff = equal_opportunity_difference(y_test, y_pred, prot_attr='race', pos_label = 0)

print(f'[Baseline] The equal opportunity difference is {eq_opp_diff: .2}')
```

Since the equal opportunity difference is below 0 and beyond -0.1, the algorithm unfairly benefits white defendants.

### Plot Fairness Metrics (Baseline)

```{python group fairness metrics plot}
plot_fair_metrics([0, 0, 0, 0], '')
plt.show()
```

# Pre-Processing Approaches

## Reweighing

```{python pre-processing set up, include = FALSE}
lr = LogisticRegressionCV(solver='lbfgs')
rew = ReweighingMeta(estimator=lr, reweigher=Reweighing('race'))
rew.fit(X_train, y_train)
y_pred_REW = rew.predict(X_test)
```

```{python pre-processing accuracy score} 
acc_REW = accuracy_score(y_test, y_pred_REW)
print(f'[Reweighting] The test accuracy of the algorithm is: {acc_REW: .2%}')
```

```{python pre-processing confusion matrix}
cf_matrix = confusion_matrix(y_test, y_pred_REW)
make_confusion_matrix(cf_matrix, "[Reweighting]")
plt.show()
```

The highest percentage in this confusion matrix is for the true negative rate, those who were predicted to not recidivate and actually did not, at 36.24%, 0.83% higher than the true negatives in the baseline model. This means that the re-weighing model marginally improved the baseline model accuracy. The true positive rate, or those who were predicted to recidivate and did recidivate, is 24.72%, 1.13% lower than that of the baseline model. This means that the re-weighing model slightly lowered the accuracy of the baseline model. The false negative and false positive rates, or the predictions that proved to be incorrect, are 23.20% and 15.85%, respectively. The re-weighing model lowered the accuracy of the false negative rate by 1.14% and improved the accuracy of the false positive rate by 0.83%. We are most concerned about the false positivesthat mean that defendants who are predicted to recidivate and do not actually recidivate. While there is a slight accuracy improvement in the false positive rate, it is still relatively high at 15.85%, indicating that we should further precede with additional bias mitigation techniques.


```{python pre-processing metrics per group}
metrics_per_group(y_test, y_pred_REW)
plt.show()
```

### Statistical Parity Difference

```{python pre-processing statistical parity difference}
stat_par_diff_RW = statistical_parity_difference(y_test, y_pred_REW, prot_attr='race', pos_label = 0)

print(f'[Reweighting] The statistical parity difference is {stat_par_diff_RW: .2}')
```

This is a large improvement over our baseline model, which was -0.14. It still implies a slight benefit for white individuals, but it is in the range of -0.1 and 0.1.

### Equal Opportunity Difference

```{python pre-processing equal opportunity difference}
eq_opp_diff_RW = equal_opportunity_difference(y_test, y_pred_REW, prot_attr='race', pos_label = 0)

print(f'[Reweighting] The equal opportunity difference is {eq_opp_diff_RW: .2}')
```

This is also an improvement from the baseline, as it is now between -0.1 and 0.1. It also now implies a slight benefit for Black defendants.

### Average Odds Difference

```{python pre-processing average odds difference}
avg_odds_diff_RW = average_odds_difference(y_test, y_pred_REW, prot_attr='race', pos_label = 0)

print(f'[Reweighting] The average odds difference is {avg_odds_diff_RW: .2}')
```

This is also a vast improvement from -0.44 to 0.014. Since it is above 1 and below 0.1, it implies a slight benefit for Black defendants, but not an unfair benefit. 

### Disparate Impact Ratio
```{python pre_processing disparate impact ratio}
disp_impact_ratio_RW = disparate_impact_ratio(y_test, y_pred_REW, prot_attr='race', priv_group = 1, pos_label = 0)

print(f'[Reweighting] The disparate impact ratio is {disp_impact_ratio_RW: .2}')
```

The reweighting approach also improves the disparate impact ratio, bringing the value almost to 1, which conveys absolute fairness.

### Plot Fairness Metrics (Reweighting)

```{python pre-processing fairness metrics plot}
plot_fair_metrics([stat_par_diff_RW, eq_opp_diff_RW, avg_odds_diff_RW, disp_impact_ratio_RW], 'Reweighting')
plt.show()
```

```{python pre-processing disparate impact remover}
#lr = LogisticRegressionCV(solver='lbfgs')
#rew = ReweighingMeta(estimator=lr, reweigher=Reweighing('race'))
#rew.fit(X_train, y_train)
#y_pred_REW = rew.predict(X_test)

# diremover = DisparateImpactRemover(repair_level=1.0, sensitive_attribute="race")
#diremover.fit(X_train, y_train)
#y_pred_diremover = diremover.predict(X_test)

# diremover_rp = diremover.fit_transform(X_test)
# AttributeError: 'DataFrame' object has no attribute 'features' (probem with .fit_transform function)
# .fit_transform() angry because of X_test... 
```

# In-Processing Approaches

```{python in-processing set up}
df_train = X_train.copy()
df_train["two_year_recid"] = y_train
df_train.rename(columns={6:'race'}, inplace=True)
df_train = df_train.reset_index(drop=True)

df_test = X_test.copy()
df_test["two_year_recid"] = y_test
df_test = df_test.reset_index(drop=True)
df_test.rename(columns={6:'race'}, inplace=True)
```

```{python in-processing training}
train_BLD = BinaryLabelDataset(favorable_label='0',
                                unfavorable_label='1',
                                df=df_train,
                                label_names=['two_year_recid'],
                                protected_attribute_names=['race'])

test_BLD = BinaryLabelDataset(favorable_label='0',
                                unfavorable_label='1',
                                df=df_test,
                                label_names=['two_year_recid'],
                                protected_attribute_names=['race'])
```

## Adversarial Debiasing
```{python adversarial debiasing}
# from aif360.algorithms.inprocessing import AdversarialDebiasing
# import tensorflow as tf
# 
# adv_deb = AdversarialDebiasing(unpr)
# #adv_deb.fit(X_train, y_train)
# #y_pred_AD = adv_deb.predict(X_test)
# #adv_deb.sess_.close()
```


## Prejudice Remover
```{python prejudice remover}
from aif360.algorithms.inprocessing import PrejudiceRemover

prej = PrejudiceRemover(eta = 1.0, sensitive_attr='race', class_attr='two_year_recid')
prej.fit(train_BLD)
y_pred_PREJ = prej.predict(test_BLD)
y_pred_PREJ = y_pred_PREJ.labels.flatten()
```

```{python prejudice accuracy score}
acc_PREJ = accuracy_score(y_test, y_pred_PREJ)
print(f'[Prejudice Remover] The test accuracy of the algorithm is: {acc_PREJ: .2%}')
```

```{python prejudice remover confusion matrix}
cf_matrix = confusion_matrix(y_test, y_pred_PREJ)
make_confusion_matrix(cf_matrix, "[Prejudice Remover]")
plt.show()
```

```{python prejudice remover metrics per group}
metrics_per_group(y_test, y_pred_PREJ)
plt.show()
```

### Statistical Parity Difference

```{python prejudice remover statistical parity difference}
stat_par_diff_PREJ = statistical_parity_difference(y_test, y_pred_PREJ, prot_attr='race', pos_label = 0)

print(f'[Prejudice Remover] The statistical parity difference is {stat_par_diff_PREJ: .2}')
```

### Equal Opportunity Difference

```{python prejudice remover equal opportunity difference}
eq_opp_diff_PREJ = equal_opportunity_difference(y_test, y_pred_PREJ, prot_attr='race', pos_label = 0)

print(f'[Prejudice Remover] The equal opportunity difference is {eq_opp_diff_PREJ: .2}')
```

### Average Odds Difference

```{python prejudice remover average odds difference}
avg_odds_diff_PREJ = average_odds_difference(y_test, y_pred_PREJ, prot_attr='race', pos_label = 0)

print(f'[Prejudice Remover] The average odds difference is {avg_odds_diff_PREJ: .2}')
```

### Disparate Impact Ratio
```{python prejudice remover disparate impact ratio}
disp_impact_ratio_PREJ = disparate_impact_ratio(y_test, y_pred_PREJ, prot_attr='race', priv_group = 1, pos_label = 0)

print(f'[Prejudice Remover] The disparate impact ratio is {disp_impact_ratio_PREJ: .2}')
```

### Plot Fairness Metrics (Prejudice Remover)

```{python prejudice remover fairness metrics plot}
plot_fair_metrics([stat_par_diff_PREJ, eq_opp_diff_PREJ, avg_odds_diff_PREJ, disp_impact_ratio_PREJ], 'Prejudice Remover')
plt.show()
```

# Post-Processing

```{python post-processing calibrated equal odds set up, include = FALSE}
from aif360.algorithms.postprocessing import EqOddsPostprocessing
from aif360.sklearn.postprocessing import CalibratedEqualizedOdds, PostProcessingMeta

pp = CalibratedEqualizedOdds('race', cost_constraint='fnr', random_state=1234567)
ceo = PostProcessingMeta(estimator=lr, postprocessor=pp, random_state=1234567)
ceo.fit(X_train, y_train)
y_pred_CEO = ceo.predict(X_test)
y_proba_CEO = ceo.predict_proba(X_test)
```

```{python post-processing accuracy score}
acc_CEO = accuracy_score(y_test, y_pred_CEO)
print(f'[Calibrated Equalized Odds] The test accuracy of the algorithm is: {acc_CEO: .2%}')
```

```{python post-processing calibrated equal odds}
cf_matrix = confusion_matrix(y_test, y_pred_CEO)
make_confusion_matrix(cf_matrix, "[Calibrated Equalized Odds]")
plt.show()
```


```{python post-processing equal odds needs fix} 
#ep = EqOddsPostprocessing(unprivileged_groups=0, privileged_groups= 1, seed =1234567)
#eo = PostProcessingMeta(estimator=lr, postprocessor=ep, random_state=1234567)
#eo.fit(train_BLD, test_BLD)
#y_pred_EO = eo.predict(test_BLD)
#y_proba_EO = eo.predict_(test_BLD)
```

### Statistical Parity Difference

```{python post-processing stat parity difference}
stat_par_diff_CEO = statistical_parity_difference(y_test, y_pred_CEO, prot_attr='race', pos_label = 0)

print(f'[Calibrated Equalized Odds] The statistical parity difference is {stat_par_diff_CEO: .2}')
```

### Equal Opportunity Difference

```{python post-processing equal opportunity diff}
eq_opp_diff_CEO = equal_opportunity_difference(y_test, y_pred_CEO, prot_attr='race', pos_label = 0)

print(f'[Calibrated Equalized Odds] The equal opportunity difference is {eq_opp_diff_CEO: .2}')
```

### Average Odds Difference

```{python post-processing average odds difference}
avg_odds_diff_CEO = average_odds_difference(y_test, y_pred_CEO, prot_attr='race', pos_label = 0)

print(f'[Calibrated Equalized Odds] The average odds difference is {avg_odds_diff_CEO: .2}')
```

### Disparate Impact Ratio

```{python post_processing disparate impact ratio}
disp_impact_ratio_CEO = disparate_impact_ratio(y_test, y_pred_CEO, prot_attr='race', priv_group = 1, pos_label = 0)

print(f'[Calibrated Equalized Odds] The disparate impact ratio is {disp_impact_ratio_CEO: .2}')
```

### Plot Fairness Metrics (Calibrated Equalized Odds)

```{python post-processing fairness metrics plot}
plot_fair_metrics([stat_par_diff_CEO, eq_opp_diff_CEO, avg_odds_diff_CEO, disp_impact_ratio_CEO], 'Calibrated Equalized Odds')
plt.show()
```

