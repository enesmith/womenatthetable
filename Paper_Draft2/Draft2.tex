%  LaTeX support: latex@mdpi.com
%  In case you need support, please attach all files that are necessary for compiling as well as the log file, and specify the details of your LaTeX setup (which operating system and LaTeX version / tools you are using).

%=================================================================
\documentclass[water,article,submit,moreauthors,pdftex]{mdpi}

% If you would like to post an early version of this manuscript as a preprint, you may use preprint as the journal and change 'submit' to 'accept'. The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%% Some pieces required from the pandoc template
\setlist[itemize]{leftmargin=*,labelsep=5.8mm}
\setlist[enumerate]{leftmargin=*,labelsep=4.9mm}


%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, aerospace, agriculture, agriengineering, agronomy, algorithms, animals, antibiotics, antibodies, antioxidants, applsci, arts, asc, asi, atmosphere, atoms, axioms, batteries, bdcc, behavsci , beverages, bioengineering, biology, biomedicines, biomimetics, biomolecules, biosensors, brainsci , buildings, cancers, carbon , catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, children, cleantechnol, climate, clockssleep, cmd, coatings, colloids, computation, computers, condensedmatter, cosmetics, cryptography, crystals, dairy, data, dentistry, designs , diagnostics, diseases, diversity, drones, econometrics, economies, education, electrochem, electronics, energies, entropy, environments, epigenomes, est, fermentation, fibers, fire, fishes, fluids, foods, forecasting, forests, fractalfract, futureinternet, futurephys, galaxies, games, gastrointestdisord, gels, genealogy, genes, geohazards, geosciences, geriatrics, hazardousmatters, healthcare, heritage, highthroughput, horticulturae, humanities, hydrology, ijerph, ijfs, ijgi, ijms, ijns, ijtpp, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jcdd, jcm, jcp, jcs, jdb, jfb, jfmk, jimaging, jintelligence, jlpea, jmmp, jmse, jnt, jof, joitmc, jpm, jrfm, jsan, land, languages, laws, life, literature, logistics, lubricants, machines, magnetochemistry, make, marinedrugs, materials, mathematics, mca, medicina, medicines, medsci, membranes, metabolites, metals, microarrays, micromachines, microorganisms, minerals, modelling, molbank, molecules, mps, mti, nanomaterials, ncrna, neuroglia, nitrogen, notspecified, nutrients, ohbm, particles, pathogens, pharmaceuticals, pharmaceutics, pharmacy, philosophies, photonics, physics, plants, plasma, polymers, polysaccharides, preprints , proceedings, processes, proteomes, psych, publications, quantumrep, quaternary, qubs, reactions, recycling, religions, remotesensing, reports, resources, risks, robotics, safety, sci, scipharm, sensors, separations, sexes, signals, sinusitis, smartcities, sna, societies, socsci, soilsystems, sports, standards, stats, surfaces, surgeries, sustainability, symmetry, systems, technologies, test, toxics, toxins, tropicalmed, universe, urbansci, vaccines, vehicles, vetsci, vibration, viruses, vision, water, wem, wevj

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by:
% abstract, addendum, article, benchmark, book, bookreview, briefreport, casereport, changes, comment, commentary, communication, conceptpaper, conferenceproceedings, correction, conferencereport, expressionofconcern, extendedabstract, meetingreport, creative, datadescriptor, discussion, editorial, essay, erratum, hypothesis, interestingimages, letter, meetingreport, newbookreceived, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, supfile, technicalnote, viewpoint
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
\firstpage{1}
\makeatletter
\setcounter{page}{\@firstpage}
\makeatother
\pubvolume{xx}
\issuenum{1}
\articlenumber{5}
\pubyear{2019}
\copyrightyear{2019}
%\externaleditor{Academic Editor: name}
\history{Received: date; Accepted: date; Published: date}
\updates{yes} % If there is an update available, un-comment this line

%% MDPI internal command: uncomment if new journal that already uses continuous page numbers
%\continuouspages{yes}

%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, calc, indentfirst, fancyhdr, graphicx, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, amsthm, hyphenat, natbib, hyperref, footmisc, geometry, caption, url, mdframed, tabto, soul, multirow, microtype, tikz

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Does the COMPAS Needle Always Point Towards Equity? Finding
Fairness in the COMPAS Risk Assessment Algorithm: A Case Study}

% Authors, for the paper (add full first names)
\Author{Amrita Acharya$^{1}$, Dianne Caravela$^{1}$, Eunice
Kim$^{1}$, Emma Kornberg$^{1}$, Elisabeth Nesmith$^{1}$}

% Authors, for metadata in PDF
\AuthorNames{Amrita Acharya, Dianne Caravela, Eunice Kim, Emma
Kornberg, Elisabeth Nesmith}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Statistical and Data Sciences Smith College Northampton, MA
01063; \href{mailto:bbaumer@smith.edu}{\nolinkurl{bbaumer@smith.edu}}\\
}
% Contact information of the corresponding author
\corres{Correspondence: \href{mailto:aacharya@smith.edu}{\nolinkurl{aacharya@smith.edu}},
\href{mailto:dcaravela@smith.edu}{\nolinkurl{dcaravela@smith.edu}},
\href{mailto:ekim89@smith.edu}{\nolinkurl{ekim89@smith.edu}},
\href{mailto:ekornberg@smith.edu}{\nolinkurl{ekornberg@smith.edu}},
\href{mailto:enesmith@smith.edu}{\nolinkurl{enesmith@smith.edu}}}

% Current address and/or shared authorship








% The commands \thirdnote{} till \eighthnote{} are available for further notes

% Simple summary

% Abstract (Do not insert blank lines, i.e. \\)
\abstract{A variety of disciplines use risk assessment instruments to
help humans make data-driven decisions. Northpointe, a software company,
created an algorithmic risk assessment instrument known as the
Correctional Offender Management Profiling for Alternative Sanctions
(COMPAS). COMPAS uses various behavioral and psychological metrics
related to recidivism to assist justice systems in assessing a
defendant's potential recidivism risk. \citet{angwin2016machine}
published a ProPublica article in which they conclude that the racial
biases in the criminal justice system are reflected in the COMPAS
recidivism risk scores. In response, \citet{equivant_response_2016}
published a rebuttal on behalf of Northpointe defending the COMPAS
algorithm and refuting \citet{angwin2016machine}'s allegation of racial
bias. Using a human rights framework adopted from the organizations
Women at the Table and AI Fairness 360, we use debiasing algorithms and
fairness metrics to analyze the argument between Northpointe and
ProPublica and determine whether and to what extent there is racial bias
in the COMPAS algorithm. All four group fairness metrics determine that
the COMPAS algorithm favors white defendants over Black defendants.}

% Keywords

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences:
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:
%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%\setcounter{secnumdepth}{4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% From pandoc table feature
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}



\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The Correctional Offender Management Profiling for Alternative Sanctions
(COMPAS) algorithm was created by the private, for-profit company
Northpointe (now known by its parent company
\href{https://www.equivant.com/faq/}{equivant}), to predict defendants'
risk of recidivism. It generates a decile score that classifies
defendants' risk of recidivism as either low, medium, or high
\citep{angwin2016machine}. Jurisdictions across the United States use
the COMPAS risk assessment instrument, including but not limited to the
\href{https://doccs.ny.gov/system/files/documents/2020/11/8500.pdf}{New
York},
\href{https://hdsr.mitpress.mit.edu/pub/hzwo7ax4/release/4}{Massachusetts},
\href{https://hdsr.mitpress.mit.edu/pub/hzwo7ax4/release/4}{Michigan},
\href{https://hdsr.mitpress.mit.edu/pub/hzwo7ax4/release/4}{California},
and \href{https://doc.wi.gov/Pages/AboutDOC/COMPAS.aspx}{Wisconsin}
Departments of Corrections.

Due to the proprietary nature of the COMPAS algorithm, it is unknown how
exactly these recidivism risk scores are calculated. However, a
\href{https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE\#document/p5/a296598}{sample
COMPAS Risk Assessment Survey} has been made publicly available,
revealing the algorithm's input information. \citet{angwin2016machine}
critiques this survey for using proxy variables for race that do not
explicitly factor in a defendant's race but heavily imply it, allowing
Northpointe to claim that their algorithm is free of racial bias. For
example, the COMPAS risk assessment survey asks screeners to speculate
if a defendant might be affiliated with a gang. It also asks if a
defendant has any friends or family members who have been crime victims.
Although these questions do not directly ask about race, they do not
take into account the pervasive nature of systemic racism that
infiltrates every aspect of the lives of marginalized people, thereby
indirectly asking about race.

\citet{angwin2016machine} analyzes the methods and algorithms used by
Northpointe in their COMPAS risk score assessment algorithm and uncovers
racial biases in defendants' scores \citep{angwin2016machine}. They find
that ``the algorithm {[}is{]} somewhat more accurate than a coin flip,''
a worrisome level of accuracy given the potential impact its
determinations may have on real people's lives.
\citet{angwin2016machine} specifically investigate the distribution of
COMPAS scores by decile among Black and white defendants. They write:
``The analysis also {[}shows{]} that even when controlling for prior
crimes, future recidivism, age, and gender, Black defendants {[}are{]}
45 percent more likely to be assigned higher risk scores than white
defendants'' \citep{larson2016we}. After examining the fairness metric
statistical parity difference, \citet{angwin2016machine} conclude that
the algorithm is racially biased \citep{larson2016we}.

\citet{equivant_response_2016}, on behalf of Northpointe, deny the
allegations of racial bias and offer their own analyses based on
different fairness metrics in rebuttal \citep{equivant_response_2016}.
\citet{angwin2016machine} maintain that there are biases in the outcome
values, protected attributes, and covariates during
\citet{equivant_response_2016}'s data processing phase. ProPublica
collaborators \citet{larson2016we} account for these biases in their
analyses. In their response, \citet{equivant_response_2016} highlight
that \citet{angwin2016machine} did not account for base rates of
recidivism in their analysis, which are important initial percentages
without the presence of other information.

\href{https://www.womenatthetable.net/}{Women at the Table}, the sponsor
organization for this project, is ``a growing, global gender equality \&
democracy CSO based in Geneva, Switzerland focused on advancing feminist
systems change by using the prism of technology, innovation \& AI
exercising leverage points in technology, the economy, sustainability \&
democratic governance.'' We are collaborating with the organization on
its AI \& Equality \citep{noauthor_ai_nodate} initiative, tasked with
de-biasing the COMPAS algorithm \citep{aif360-oct-2018} and producing a
corresponding data story that will be added to its library.

Our project builds on Women at the Table's various de-biasing algorithms
used in its AI \& Equality Human Rights Toolbox to conduct our own
analyses on the COMPAS data set. Based on this analysis, we employ a
human rights framework to contribute to the ProPublica and Northpointe
debate and investigate whether or to what extent there is racial bias in
the COMPAS algorithm. With a solid understanding of the two sides, we
aim to pinpoint the shortcomings of both arguments and correct them in
our analyses. We will use various de-biasing techniques and fairness
metrics to evaluate the level of bias present in the COMPAS data and our
algorithm. We will summarize our results using the JupyterNotebook
framework from Women at the Table, to be used by members of the
organization to teach in a workshop setting. We hope that our findings
will highlight the importance of checking statistical analyses using
varied methods and contribute to the ongoing discussion of the effects
of machine biases in the justice system.

\hypertarget{data}{%
\section{Data}\label{data}}

The data we are using for this project is the COMPAS General Recidivism
Risk Scores data set from the AI Fairness 360 (AIF360) toolkit
\citep{aif360-oct-2018}, which does the same initial pre-processing as
ProPublica. The raw data has 6,167 rows and each row represents an
arrest charge for a defendant. AIF360's COMPAS data includes the
defendant's age, race, sex, prior charges, what they were charged with,
and whether or not the defendant ultimately recidivated within a
two-year period after their arrest. For the purposes of our project,
which endeavors to evaluate the differing effects of the COMPAS
algorithm on white defendants versus Black defendants, we have filtered
the data to only include individuals whose race is listed as Caucasian
or African-American. Our data therefore has 5,273 rows (Figure
\ref{fig:table snip}), with the distributions of age (Figure
\ref{fig:age plot}), prior charges (Figure \ref{fig:prior plot}), and
two-year recidivism by race (Figure \ref{fig:recidivism mosaic}) and
(Table \ref{tab:recid table}) shown below.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{../images/table_snippet} 

}

\caption{A snippet of the data set we will be using, containing information on a defendant's age, sex, race, criminal history, charge degree, charge description, and two-year recidivism outcome.}\label{fig:table snip}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{../images/age_race_plot_new} 

}

\caption{The purple curve shows the distribution of the ages of Black defendants, and the green curve shows the distribution of the ages of white defendants. The probability of a defendant's age being between two points on the x-axis is the total shaded area of the curve under the two points. The purple dotted line represents the median age of Black defendants (29 years) and the green dotted line represents the median age of white defendants (35 years). For both groups, the majority of defendants are relatively young, but this is especially noticeable for Black defendants.}\label{fig:age plot}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{../images/prior_charges_plot} 

}

\caption{Black defendants, particularly men, are more likely to have a greater count of prior charges than white defendants. Male defendants have a higher number of prior charges than do female defendants. Though we do not know for sure which information goes into the COMPAS algorithm, it is likely that a defendant with prior charges will be coded as a having a higher risk of recidivism. Thus, by looking at the racial discrepancies in prior charges we can already see potential bias in the algorithm.}\label{fig:prior plot}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{../images/recidivism_mosaic} 

}

\caption{When we divide the data into Black and white defendants, we can see that Black defendants recidivate more than white defendants and Black defendants are more likely to recidivate than not recidivate. 39.14\% of white defendants did recidivate within two years compared to 52.35\% of Black defendants. We can also see that there are more Black defendants in the dataset overall.}\label{fig:recidivism mosaic}
\end{figure}

\begin{longtable}[]{@{}lrrr@{}}
\caption{Incidence of recidivism by race, illustrating how a much
greater proportion (\textgreater{} 50\%) of Black defendants recidivated
than their white counterparts. \label{tab:recid table}}\tabularnewline
\toprule
Two Year Recidivism by Race & Recidivated & Survived &
Total\tabularnewline
\midrule
\endfirsthead
\toprule
Two Year Recidivism by Race & Recidivated & Survived &
Total\tabularnewline
\midrule
\endhead
African American & 1661 & 1512 & 3173\tabularnewline
Caucasian & 822 & 1278 & 2100\tabularnewline
Total & 2483 & 2790 & 5273\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{methods}{%
\section{Methods}\label{methods}}

AI Fairness 360 (AIF360) is an open-source Python toolkit that seeks
``to help facilitate the transition of fairness research algorithms to
use in an industrial setting and to provide a common framework for
fairness researchers to share and evaluate algorithms''
\citep{aif360-oct-2018}. It contains multiple data sets, including the
COMPAS data set that accompanied \citet{angwin2016machine}.

The AIF360 toolkit contains various group and individual fairness
metrics as well as pre-processing, in-processing, and post-processing
algorithms that we used to debias the COMPAS algorithm
\citep{aif360-oct-2018}. We researched the definitions and applications
of different fairness metrics \citep{ashokan2021fairness} to determine
which metric would be most appropriate for our project. We chose to look
at four group fairness metrics instead of individual fairness metrics.
Group fairness metrics take into account the attributes of a whole group
as opposed to just one individual in the group, allowing us to represent
systemic issues. In general, group fairness metrics require that the
unprivileged group is treated similarly to the privileged group, whereas
individual fairness metrics require individuals to be treated
consistently \citep{kypraiou_what_2021}. Group and individual metrics
work in opposition of one another, meaning that when group fairness
improves, individual fairness gets worse \citep{kypraiou_what_2021}.

\hypertarget{statistical-parity-difference}{%
\subsection{Statistical Parity
Difference}\label{statistical-parity-difference}}

This metric measures the difference between privileged and marginalized
groups' likelihood to get a particular outcome. The ideal value of this
metric is 0. Fairness for this metric is between -0.1 and 0.1. A
negative value means there is higher benefit for the privileged group
(in this case, white defendants).

\[P(\hat{Y}=1|D=Unprivileged) - P(\hat{Y}=1|D=Privileged)\]

\hypertarget{disparate-impact-ratio}{%
\subsection{Disparate Impact Ratio}\label{disparate-impact-ratio}}

This metric is the ratio of how often the favorable outcome occurs in
one group versus the other. In the case of recidivism, this is the ratio
of how many white defendants are predicted to not recidivate compared to
how many black defendants are predicted to not recidivate. A value of 1
means that the ratio is exactly 1:1. Less than 1 means the privileged
group (white defendants) benefits, while a value greater than 1 means
the unprivileged group (Black defendants) benefits. According to AIF360,
a ratio between 0.8 to 1.25 is considered fair \citep{Ronaghan2019AI}.

\[\frac{P(\hat{Y}=1|D=Unprivileged)}{P(\hat{Y}=1|D=Privileged)}\]

\hypertarget{equal-opportunity-difference}{%
\subsection{Equal Opportunity
Difference}\label{equal-opportunity-difference}}

The
\href{https://developers.google.com/machine-learning/glossary/fairness\#e}{equal
opportunity difference} metric is computed as the difference of true
positive rates between the unprivileged and the privileged groups. The
true positive rate is the ratio of true positives to the total number of
actual positives for a given group.

The ideal value is 0. A value less than 0 implies higher benefit for the
privileged group and a value greater than 0 implies higher benefit for
the unprivileged group. Fairness for this metric is between -0.1 and 0.1
\citep{aif360-oct-2018}.

This metric is best used when it is very important to catch positive
outcomes while false positives are not exceptionally problematic
\citep{Cortez2019How}. This is not the case for the COMPAS data set, as
false positives mean extra jail time for someone who will not actually
re-offend.

\[TPR_{D = Unprivileged} - TPR_{D = Privileged}\]

\hypertarget{average-odds-difference}{%
\subsection{Average Odds Difference}\label{average-odds-difference}}

This metric returns the average difference in false positive rate and
true positive rate for the privileged and unprivileged groups. A value
of 0 indicates equality of odds, and a value below 0 implies benefit for
the privileged group. Equality of odds is achieved in the case of
recidivism when the proportion of people who were predicted to
recidivate and did recidivate is equal (true positive rate) for both
Black and white defendants AND the proportion of people who were
predicted to recidivate and did not recidivate (false positive rate) is
equal for both Black and white defendants \citep{aif360-oct-2018}.

\[\frac{1}{2}\left[(FPR_{D = Unprivileged} - FPR_{D = Privileged}) + \underbrace{(TPR_{D = Unprivileged} - TPR_{D = Privileged})}_\textrm{Equal Opportunity Difference}\right]\]

For the next step of our experiment, we need to determine where in the
data science pipeline we can mitigate the most bias, using
pre-processing, in-processing, and post-processing de-biasing
algorithms. These are all based on using predictive models to figure out
how we can ``fix'' the bias that is present.

Pre-processing refers to mitigating bias within the training data, and
it is the most flexible method because it has not yet trained a model
that may carry assumptions about the data. It is important to keep in
mind that pre-processing prevents assumptions in the modeling, but does
not account for the bias in data collection. Training data is where bias
is most likely to be introduced. We use the reweighing pre-processing
algorithm from AIF360 which assigns weights to the data. ``The advantage
of this approach is, instead of modifying the labels, it assigns
different weights to the examples based upon their categories of
protected attribute and outcome such that bias is removed from the
training dataset. The weights are based on frequency counts. However as
this technique is designed to work only with classifiers that can handle
row-level weights, this may limit your modeling options''
\citep{Ronaghan2019AI}. We used a logistic regression model for this
algorithm, as it is the easiest to interpret in the given context. After
running the fairness metrics using the pre-processing algorithm, we were
able to compare our results to the baseline metrics from the previous
section.

In-processing mitigates bias in classifiers while building a model. A
classifier ``is an algorithm that automatically orders or categorizes
data into one or more sets'' \citep{baxter2021AI}. The in-processing
technique we use is the prejudice remover algorithm, which accounts for
the fairness metric as part of the input and returns a classifier
optimized by that particular metric. In order to do this, we first
needed to convert our data frame into a data type called a
BinaryLabelDataset. Similarly to pre-processing, we compared the results
of our in-processing methods with both the baseline and the
pre-processing to gauge which method so far has better individual or
group fairness.

The prejudice remover is a method for reducing indirect prejudice
(i.e.~how COMPAS is racially biased because it uses proxy variables for
race). The prejudice remover implements two different regularizers, one
to avoid overfitting and one to enforce fair classification. The
prejudice remover regularizer works by minimizing the prejudice index, a
mathematical equation for quantifying fairness defined by Kamishima et.
al.~This in turn enforces a classifier's independence from sensitive
information (e.g., race).

Our last approach, post-processing bias mitigation, is used after
training a model. Post-processing algorithms equalize the outcomes
(i.e., predicted classification values) to mitigate bias instead of
adjusting the classifier or the training data \citep{baxter2021AI}. We
use calibrated equalized odds, which ``optimizes over calibrated
classifier score outputs to find probabilities with which to change
output labels with an equalized odds objective'' {[}insert aif360
website citation here{]}. An equalized odds objective constrains
classification algorithms such that no error type (false-positive or
false-negative) disproportionately affects any population subgroup; both
groups, in our case both white and Black defendants, should have the
same false-positive and false-negative rates. Through the calibrated
equalized odds method, we want to decrease bias while also maintaining
calibration \citep{pleiss2017fairness}.
\href{https://medium.com/analytics-vidhya/calibration-in-machine-learning-e7972ac93555}{Calibration}
refers to improving a model so that the distribution of predicted
outcomes is similar to the distribution of observed probability in the
training data.

\hypertarget{results}{%
\section{Results}\label{results}}

With our baseline model, we ran the four different group fairness
metrics we chose and compared the results (pictured in Table
\ref{tab:baseline metrics table}).

The statistical parity difference is -0.14. This indicates that there is
a large difference between white and Black defendants regarding whether
or not they recidivate. The algorithm unfairly benefits white defendants
over Black defendants. Disparate impact ratio is 0.47. The ratio of
white defendants predicted to not recidivate to the Black defendants
predicted to not recidivate is 0.47. A ratio between 0.8 and 1.25 is
considered fair, therefore the algorithm unfairly benefits white
defendants.

Average odds difference is -0.44. The average difference in false
positive rates and true positive rates for white and Black defendants is
-0.44. Values less than zero are considered in favor of the privileged
group, so the algorithm unfairly benefits white defendants.

Equal opportunity difference is -0.41. The difference of true positive
rates between the Black and white groups is -0.41. A value less than 0
indicates a benefit to the privileged group, so the algorithm unfairly
benefits white defendants. The value is substantially less than -0.1,
which indicates that the algorithm benefits white defendants.

All four group fairness metrics determine that the COMPAS algorithm
favors white defendants over Black defendants. Although the magnitudes
of the various fairness metrics are different, all of the fairness
metrics are not within their respective fairness thresholds. Our goal is
to use pre-processing, in-processing, and post-processing algorithms in
the AIF360 toolkit to see if we can make COMPAS fair at all.

\begin{longtable}[]{@{}llll@{}}
\caption{Results of Baseline Fairness Metric Analysis
\label{tab:baseline metrics table}}\tabularnewline
\toprule
Fairness Metric & Ideal Value & Baseline Value & Benefited
Group\tabularnewline
\midrule
\endfirsthead
\toprule
Fairness Metric & Ideal Value & Baseline Value & Benefited
Group\tabularnewline
\midrule
\endhead
Statistical Parity Difference & 0 & -0.14 & White
Defendants\tabularnewline
Disparate Impact Ratio & 1 & 0.47 & White Defendants\tabularnewline
Average Odds Difference & 0 & -0.44 & White Defendants\tabularnewline
Equal Opportunity Difference & 0 & -0.41 & White
Defendants\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{pre-processing-approach}{%
\subsection{Pre-Processing Approach}\label{pre-processing-approach}}

After running the reweighing algorithm, our fairness metrics are -0.015
for statistical parity difference, 0.015 for equal opportunity
difference, 0.014 for average odds difference, and 0.98 for disparate
impact ratio. Overall, these fairness metrics show that the
pre-processing algorithm reweighing improves the bias in the COMPAS
algorithm.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %% optional
% \supplementary{The following are available online at www.mdpi.com/link, Figure S1: title, Table S1: title, Video S1: title.}
%
% % Only for the journal Methods and Protocols:
% % If you wish to submit a video article, please do so with any other supplementary material.
% % \supplementary{The following are available at www.mdpi.com/link: Figure S1: title, Table S1: title, Video S1: title. A supporting video article is available at doi: link.}

\vspace{6pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here.

%=====================================
% References, variant A: internal bibliography
%=====================================
%\reftitle{References}
%\begin{thebibliography}{999}
% Reference 1
%\bibitem[Author1(year)]{ref-journal}
%Author1, T. The title of the cited article. {\em Journal Abbreviation} {\bf 2008}, {\em 10}, 142--149.
% Reference 2
%\bibitem[Author2(year)]{ref-book}
%Author2, L. The title of the cited contribution. In {\em The Book Title}; Editor1, F., Editor2, A., Eds.; Publishing House: City, Country, 2007; pp. 32--58.
%\end{thebibliography}

% The following MDPI journals use author-date citation: Arts, Econometrics, Economies, Genealogy, Humanities, IJFS, JRFM, Laws, Religions, Risks, Social Sciences. For those journals, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%=====================================
% References, variant B: external bibliography
%=====================================
\reftitle{References}
\externalbibliography{yes}
\bibliography{mybibfile.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional

%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
